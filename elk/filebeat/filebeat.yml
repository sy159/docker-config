# 具体参数配置直接看官网：https://www.elastic.co/guide/en/beats/filebeat/current/configuring-howto-filebeat.html

filebeat.inputs:
- type: filestream
  id: access_json_logs
  enabled: true
  take_over: true
  paths:
    - /var/logs/access/access.log  # 如果是写入有分隔，建议监听除了access.log以外的其他文件（/var/logs/access/access-*.log）
  # json配置
  parsers:
    - ndjson:
        target: ''  # 如果设置为空，就会把json全部展开，设置了字段就会是target.xxx
        add_error_key: true
        overwrite_keys: true  # 发生冲突时，解码的JSON 对象中的值将覆盖Filebeat通常添加的字段（类型、源、偏移量等）。
        expand_keys: false  # 是否展开所有json
  # 处理器(更多使用查看官网)
  processors:
    - timestamp:
        field: "time"  # 指定包含时间字符串的字段
        target_field: "@timestamp"  # 转换后的时间戳将存储在 "@timestamp"
        layouts:
          - '2006-01-02 15:04:05.999'  # 指定时间字符串的格式,go的format格式
        timezone: "Asia/Shanghai"  # 如果指定是UTC，那么现实的timestamp会多8h;上海就是跟你的字段一样

  fields_under_root: true  # 自定义字段将作为顶级字段存储在输出文档中，而不是分组到fields子词典下。如果自定义字段名称与Filebeat添加的其他字段名称冲突，则自定义字段将覆盖其他字段。
  fields:
    from_source: access
    env: dev

- type: filestream
  id: error_logs
  enabled: true
  take_over: true
  paths:
    - /var/logs/error/error.log  # 如果是写入有分隔，建议监听除了access.log以外的其他文件（/var/logs/access/error-*.log）
  parsers:
    - multiline:
        type: pattern
        pattern: '^[0-9]{4}-[0-9]{2}-[0-9]{2}'  # 匹配的正则表达式，匹配时间开头
        negate: true  # 是否取反，即当不以日期格式开头时
        match: after  # 匹配规则，读取的行不以日期开头时，合并到以日期开头为一行。例如堆栈异常多行信息应当合并为一行消息
  processors:
    # 解析多行日志
    - dissect:
        tokenizer: "%{create_date} %{create_time}	%{loglevel}	%{err_path}	%{message}"
        field: "message"
        target_prefix: ""  # 解析出来的字段前缀，示例：xxx.created_at
        overwrite_keys: true
    - dissect:
        when:
          has_fields: [ "err_path" ]
        tokenizer: "%{err_file}:%{err_line|integer}"
        field: "err_path"
        target_prefix: ""
        overwrite_keys: true
    - script:
        lang: javascript
        id: combine_date_time
        source: >
          function process(event) {
              var create_date = event.Get("create_date");
              var create_time = event.Get("create_time");
              if (create_date && create_time) {
                  var created_at = create_date + ' ' + create_time;
                  event.Put("created_at", created_at);
              } else {
                  event.Put("error", "Missing create_date or create_time");
              }
          }
    - timestamp:
        when:
          has_fields: [ "created_at" ]
        field: "created_at"
        target_field: "@timestamp"
        layouts:
          - '2006-01-02 15:04:05.999'  # go time formatter
        timezone: "Asia/Shanghai"  # 如果指定是UTC，那么现实的timestamp会多8h;上海就是跟你的字段一样
    - drop_fields:
        fields: [ "create_date", "create_time" , "err_path"]
  fields_under_root: true
  fields:
    from_source: error
    env: dev

# 定义模板的相关信息，可以确保所有索引都有一致的设置和映射。（启动以后同步的是数据流，不启动是到索引）
setup.template.enabled: false
#setup.template.name: "test_log"
#setup.template.pattern: "test-log-*"
#setup.template.overwrite: false  # 如果有就覆盖
#setup.template.settings:
#  index.number_of_shards: 1  # 我们是单节点所以就搞一个分片，一般分片数 * （副本数+1）为节点数
#  index.number_of_replicas: 0  # 副本数

# 生命周期管理
setup.ilm.enabled: false


output.elasticsearch:
  hosts: [ "http://192.168.0.107:9200"]  # 事件将按循环顺序分发到这些节点。如果一个节点无法访问，事件将自动发送到另一个节点，不是每个节点都发送！！！
  username: "elastic"
  password: "zx.123"
  indices:
    - index: "test-log-access-%{+yyyy.MM.dd}"  # 是否需要根据时间分割%{+yyyy.MM.dd}
      when.equals:
        from_source: "access"
    - index: "test-log-error-%{+yyyy.MM.dd}"
      when.equals:
#        fields.source: "error"
        from_source: "error"

processors:
  - drop_fields:
      fields: ["agent.ephemeral_id", "agent.name", "agent.version", "ecs.version", "host.name", "log.file.device_id",
               "log.file.inode", "log.file.path", "log.flags", "log.offset", "agent.type", "input.type"]
